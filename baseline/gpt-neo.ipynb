{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d582985-3ef8-4def-9e95-00dfc7b712d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T00:17:24.398144600Z",
     "start_time": "2024-05-03T00:17:24.387682Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import GPT2TokenizerFast, GPTNeoForCausalLM, GPTNeoConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d110423a-af0f-470d-8a06-9ebfb4f4b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\wue09\\.netrc\n"
     ]
    }
   ],
   "source": [
    "transformers.set_seed(123)\n",
    "with open('../wandb_key.txt') as f:\n",
    "    wandb.login(key = f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec337d99-d896-4410-884f-284e1762a61e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0242c21-dbe2-4faa-8b48-afd0d758d693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T00:18:48.056898800Z",
     "start_time": "2024-05-03T00:17:27.096787800Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"../data/TinyStories\")\n",
    "tokenized_dataset = load_from_disk(\"../data/TokenizedTinyStories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c578b72-c15a-44d8-a761-e0cd56ebd7bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241bfddd-f262-4e4a-aeb2-1ffcaab7e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTNeoConfig(\n",
    "\n",
    "    # number of tokens in the vocabulary \n",
    "    vocab_size = 10_000, \n",
    "    # embedding size (vector length) of each token \n",
    "    hidden_size=384, \n",
    "    # we thus have an embedding block of 512 x 10'000 parameters\n",
    "\n",
    "    # maximum sequence length, though inputs longer than `hidden_size` will be iteratively processed\n",
    "    max_position_embeddings = 512, \n",
    "\n",
    "    # number of transformer blocks. div by 2 for attention_types\n",
    "    num_layers=2, \n",
    "    # for global and local attention (GPT-Neo-specific)\n",
    "    attention_types=[[[\"global\", \"local\"], 1]], \n",
    "\n",
    "    num_heads=4,     # attention heads\n",
    "    window_size=384, # for local attention (GPT-Neo-specific)\n",
    "\n",
    "    intermediate_size=384*8, # size of 'up-projection' layer in FFN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f57d328-a49e-46a2-bcf7-fb1076f440fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding token is <pad>\n"
     ]
    }
   ],
   "source": [
    "tokenize_function = GPT2TokenizerFast.from_pretrained('../10k-tok', model_max_length=config.max_position_embeddings)\n",
    "\n",
    "assert tokenize_function.model_max_length == config.max_position_embeddings\n",
    "assert tokenize_function.vocab_size == config.vocab_size\n",
    "\n",
    "# printing this because of a bug in tokenizers (should be fixed now) https://github.com/huggingface/transformers/issues/26500\n",
    "print(f'padding token is {tokenize_function.pad_token}')\n",
    "# HF wasn't saving this nor the tokenizer's pad_token\n",
    "config.pad_token_id = tokenize_function.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221617d1-4998-43b3-8a3d-c741e5df7bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,946,368 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = GPTNeoForCausalLM(config=config)\n",
    "\n",
    "print(f'The model has {model.num_parameters():,} parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c33cd-f112-44cf-9ca9-a569950fbff6",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a346c76-2392-45f3-b6aa-1ee3acefae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(tokenized_dataset['train'][0]['input_ids']) == config.max_position_embeddings\n",
    "tokenized_dataset['train'][0]['input_ids'][-10:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2308cb15-987a-4d6a-a799-b53d87aaf3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = tokenized_dataset['train'], tokenized_dataset['validation']\n",
    "\n",
    "batch_size = 16 # TinyStories claims 80, but I am training locally on my poor M1 Air\n",
    "num_train_epochs = 1  # TinyStories doesn't mention\n",
    "gradient_accumulation_steps = 16 # TinyStories claims 16\n",
    "\n",
    "lr = 5e-4 # TinyStories claims 5e-4, higher values are preferable for smaller models\n",
    "\n",
    "_train_steps = len(train_dataset) // (batch_size * gradient_accumulation_steps)\n",
    "eval_steps = _train_steps // 10 # evaluate every 10% of training steps\n",
    "\n",
    "model_name = f'{model.num_parameters()//1e6:.1f}M-{config.num_layers}L-{config.num_heads}H-{config.hidden_size}C-{config.intermediate_size}I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b07b67f3-6ac6-41da-b826-cfea79b54060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    training for 1 epochs, 2119719 samples\n",
      "    16 batch size, 16 accumulation steps\n",
      "    gives 8280 training steps.\n",
      "\n",
      "    evaluating every 828 steps, 21990 samples \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    seed       = 42,\n",
    "    use_cpu    = False, # use GPU if available (not necessarily faster on laptops, but Apple's MPS have good support)\n",
    "    output_dir = f'./results/models/{model_name}',\n",
    "\n",
    "    # NOTE: training params\n",
    "    learning_rate    = lr,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    # Use a smaller batch size to fit into GPU RAM. \n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size  = batch_size,\n",
    "    # You should aim to have the same amount of samples per acc step, in all of your experiments!\n",
    "    # so, if you increase batch_size, decrease gradient_accumulation_steps by the same factor.\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\n",
    "    # NOTE: Evaluation params\n",
    "    # wandb is great for tracking experiments, it will even (try to) save your code nowadays\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps = eval_steps,\n",
    "    save_steps = eval_steps,\n",
    "\n",
    "    logging_first_step=True,\n",
    "    logging_steps=eval_steps,\n",
    "    report_to  = 'wandb',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset = train_dataset, \n",
    "    eval_dataset = eval_dataset,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenize_function, mlm=False),\n",
    ")\n",
    "\n",
    "# print amount of training steps, and how often the model is evaluated\n",
    "print(f'''\n",
    "    training for {num_train_epochs} epochs, {len(train_dataset)} samples\n",
    "    {batch_size} batch size, {gradient_accumulation_steps} accumulation steps\n",
    "    gives {_train_steps} training steps.\n",
    "\n",
    "    evaluating every {eval_steps} steps, {len(eval_dataset)} samples \n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc193024-e9a7-4079-ab61-868f18f9bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: eug_man (club-of-eugene). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\CSE3000 Research Project\\SparseFFN\\baseline\\wandb\\run-20240504_183853-34tw3vfn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/club-of-eugene/gpt-neo/runs/34tw3vfn' target=\"_blank\">9.0M-2L-4H-384C-3072I</a></strong> to <a href='https://wandb.ai/club-of-eugene/gpt-neo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/club-of-eugene/gpt-neo' target=\"_blank\">https://wandb.ai/club-of-eugene/gpt-neo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/club-of-eugene/gpt-neo/runs/34tw3vfn' target=\"_blank\">https://wandb.ai/club-of-eugene/gpt-neo/runs/34tw3vfn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='8280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  25/8280 03:24 < 20:23:33, 0.11 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='gpt-neo', name=model_name, config=training_args)\n",
    "trainer.train()\n",
    "trainer.save_model(f'./results/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5251bd84-d479-4ae0-bae9-a4ff840b7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f'./results/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7443cf-5910-4332-ba32-838a5bdccd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap # for pretty printing\n",
    "w = textwrap.TextWrapper(replace_whitespace=False, break_long_words=False, width=60, initial_indent='   ', subsequent_indent='  ')\n",
    "def see(text): print('\\n\\033[3m' + '\\n\\n'.join(['\\n'.join(w.wrap(line))\n",
    "                 for line in text.splitlines() if line.strip() != '']) + '\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ce37fc2-e439-44ab-b31f-cbb6b22f995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../10k-tok')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(f'results/models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872fe241-ef11-4f3a-80be-b2fed0c156cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m   <s>My nigga</s>igige and a big tree. They had a big tree.\n",
      "  They had a big tree. They had a big tree. They had fun.\n",
      "  They had fun. They had fun. They had fun. They had fun.\n",
      "\n",
      "   One day, the tree. They had a big, and the tree. They had\n",
      "  a big tree. They had a big tree. They had fun. They had\n",
      "  fun. They had a big, and the tree. They had fun. They had\n",
      "  fun. They had fun. They had fun. They had fun. They had\n",
      "  fun. They had fun.\n",
      "\n",
      "   \"Hello, the tree. They had a fun!\" Tom said. They had\n",
      "  fun!\" Tom and the tree. They had fun. They had fun. They\n",
      "  had fun. They had fun. They had fun. They had fun. They\n",
      "  had fun. They had fun. They had fun. They had fun. They\n",
      "  had fun. They had fun. They had fun. They had fun.\n",
      "\n",
      "   \"Hello, Tom said. They had fun. They had fun. They had\n",
      "  fun. They had fun. They had fun. They had fun. They had\n",
      "  fun. They had fun. They had fun. They had fun. They had\n",
      "  fun. They had fun. They had fun. They had fun. They had\n",
      "  fun. They had fun. They had fun. They had fun. They had\n",
      "  fun.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'All work and no play makes Jack a dull boy'\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=300, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0])\n",
    "\n",
    "# textwrap with indentation on every new paragraph\n",
    "see(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8950ceb6-0dc7-4f7e-99eb-d1b1100d067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./results/base.bin\", safe_serialization=False) #Replace path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5694877-50a9-4188-91fd-5c981e4d3998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
